üöÄ **Chatbot with RAG & Ollama**

This repository contains a Flask-based chatbot with Retrieval-Augmented Generation (RAG) powered by ChromaDB and Ollama for local LLM inference. It includes a Streamlit UI for interaction and is fully containerized using Docker & Docker Compose.

‚ú® **Features**

Document Upload & Processing: Extracts text from PDFs and stores chunked embeddings in ChromaDB.
Hybrid Retrieval: Uses semantic search + keyword filtering for improved response accuracy.
Ollama Integration: Runs DeepSeek-R1 1.5B locally for cost-effective inference.
Self-Verification: LLM checks its own responses for accuracy.
Direct Answer Mode: Returns exact matches without LLM generation when possible.
Streamlit Frontend: Simple, interactive UI for querying documents.
Dockerized Deployment: Run the entire system with docker-compose up.

üõ† **Tech Stack**

Backend: Flask, LangChain, ChromaDB, Ollama
Frontend: Streamlit
Infrastructure: Docker, Docker Compose

üöÄ **Getting Started**

1Ô∏è‚É£ Clone the repo  
2Ô∏è‚É£ Run `docker-compose up --build`  
3Ô∏è‚É£ Open [http://localhost:8501](http://localhost:8501) in your browser  
4Ô∏è‚É£ Upload a PDF & start chatting!  